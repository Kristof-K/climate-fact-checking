preprocessing:
  folders: ["UnitedNations", "Wikipedia", "NationalGeographic",
            "TheNewYorkTimes", "NationalOceanicAndAtmosphericAdministration"]
  lower_case: True
  min_words: 5
  mask_symbol: "\r"
encoding:
  word_based: False
  # if word_based True maximal number of words per statement,
  # otherwise maximal number of characters
  max_seq_len: 200
  max_output: 30
  start_token: "\t"
  end_token: "\n"
model_training:
  train: False
  use_generator: True
  model: "CharToCharBiLSTM"
  epochs: 25
  # save model weights every save_epochs iterations
  save_epochs: 5
  learning_rate: 0.1
  batch_size: 64
  num_neurons: 512
load_model:
  folder: "CharToCharBiLSTM_2023-11-10_14-53"
  epoch: 5