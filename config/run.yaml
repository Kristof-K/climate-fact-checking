preprocessing:
  # if file already exists, read statements from it otherwise create it
  data_file: 'wikipedia_climate_small.txt'
  folders: "WIKIPEDIA"
  lower_case: True
  min_words: 5
  mask_symbol: "<mask>"
  word_tokenizer: "nltk"
  word_based: True
  # if word_based True maximal number of words per statement,
  # otherwise maximal number of characters
  max_seq_len: 30
  # if word_based True size of output vocabulary, otherwise
  # maximal output sequence length
  max_output: 50000
  # both tokens only important for character based encoding (word based False)
  start_token: "\t"
  end_token: "\n"
model_training:
  train: False
  model: "WordvecToWordBiLSTM"
  epochs: 10
  # save model weights every save_epochs iterations
  save_epochs: 1
  learning_rate: 0.0001
  batch_size: 64
  num_neurons: [1024, 1024]
load_model:
  folder: "WordvecToWordBiLSTM_2023-11-28_09-18"
  epoch: 10